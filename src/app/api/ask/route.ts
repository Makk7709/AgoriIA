import { NextResponse } from 'next/server'
import OpenAI from 'openai'
import { pinecone } from '@/lib/pinecone'
import { getCachedResponse, setCachedResponse, generateCacheKey } from '@/lib/cache/responseCache'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
})

const SYSTEM_PROMPT = `Tu es un assistant citoyen r√©publicain, neutre, factuel, et p√©dagogue.
Ta mission : aider l'utilisateur √† comprendre les positions politiques en comparant les programmes des candidats.

R√®gles :
- Toujours rester neutre et objectif
- Citer les sources quand disponibles
- Structurer la r√©ponse en 3 parties
- Utiliser un ton r√©publicain et p√©dagogue
- R√©pondre en fran√ßais avec un style digne et chaleureux

Format de r√©ponse OBLIGATOIRE :
1. Introduction
[Contexte de la question et pr√©sentation du sujet]

2. Analyse d√©taill√©e
[Points cl√©s et analyse factuelle]
- Point 1
- Point 2
- Point 3

3. Conclusion
[Synth√®se neutre et p√©dagogique]`

export async function POST(req: Request) {
  const startTime = Date.now()
  try {
    const { question } = await req.json()
    console.log("üîé Question pos√©e :", question)

    // V√©rifier le cache de la r√©ponse
    const cacheKey = generateCacheKey(question)
    const cachedResponse = await getCachedResponse(cacheKey)
    if (cachedResponse) {
      console.log('R√©ponse trouv√©e dans le cache')
      return NextResponse.json({
        content: cachedResponse,
        fromCache: true,
        responseTime: Date.now() - startTime
      })
    }

    // G√©n√©rer l'embedding de la question
    const embeddingStart = Date.now()
    const embeddingResponse = await openai.embeddings.create({
      model: "text-embedding-ada-002",
      input: question
    })
    const queryVector = embeddingResponse.data[0].embedding
    console.log('Temps de g√©n√©ration embedding:', Date.now() - embeddingStart, 'ms')

    // Interroger Pinecone
    const pineconeStart = Date.now()
    const index = pinecone.Index(process.env.PINECONE_INDEX_NAME!)
    const pineconeResponse = await index.query({
      vector: queryVector,
      topK: 5,
      includeMetadata: true
    })
    console.log('Temps de requ√™te Pinecone:', Date.now() - pineconeStart, 'ms')

    // Construire le contexte
    const context = pineconeResponse.matches
      .map((m) => m.metadata?.summary || m.metadata?.position || "")
      .join("\n---\n")
    console.log("üìö Contexte vectoriel utilis√© :", context)

    // G√©n√©rer la r√©ponse avec OpenAI
    const openaiStart = Date.now()
    const completion = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        { role: "system", content: SYSTEM_PROMPT },
        { role: "user", content: `Question: "${question}"\n\nContexte:\n${context}` }
      ],
      temperature: 0.7,
      max_tokens: 1000
    })
    console.log('Temps de g√©n√©ration OpenAI:', Date.now() - openaiStart, 'ms')

    const response = completion.choices[0]?.message?.content

    if (!response) {
      throw new Error('Pas de r√©ponse g√©n√©r√©e')
    }

    // Mettre en cache la r√©ponse
    await setCachedResponse(cacheKey, response)

    console.log('Temps total de traitement:', Date.now() - startTime, 'ms')

    const finalResponse = {
      content: response,
      fromCache: false,
      responseTime: Date.now() - startTime
    }
    console.log("‚úÖ R√©ponse retourn√©e au client :", finalResponse)
    return NextResponse.json(finalResponse)

  } catch (error) {
    console.error('Erreur API:', error)
    return NextResponse.json(
      { error: 'Une erreur est survenue lors du traitement de votre demande' },
      { status: 500 }
    )
  }
} 